\documentclass[a4paper]{article}
\usepackage{femape}
\title{EidA Zusammenfassung}
\author{Felix Leitl}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	\section{Laufzeit}
	\subsection{\Theta-Notation}Def. $\Theta(g(n))=\{f(n):\exists c_1,c_2,n_0\geq 0, \text{ sodass } 0\leq c_1\cdot g(n)\leq c_2\cdot g(n)\forall n \geq n_0\}$ \newline \newline
		Bedeutung: 	
		\begin{enumerate}
		\item kleine Werte von $n$ sind nicht. wichtig
		\item $c_1$ und $c_2$ begrenzen $f$ nach oben und unten
	\end{enumerate} 
	\subsection{$\BigO$-Notation}
		Def. $\BigO(g(n))=\{f(n):\exists c,n_0\geq 0, \text{ sodass }0\leq f(n)\leq c\cdot g(n)\forall n\geq n_0\}$	 \newline \newline
		\begin{enumerate}
			\item $f=\Theta(g(n))\Rarr f(n)=\BigO(g(n))$
			\item $\BigO$-Notation gibt keine exakte obere Schranke an
		\end{enumerate}
		\subsection{\Omega-Notation}
			Def. $\Omega(g(n))=\{f(n):\exists c,n_0\geq 0, \text{ sodass } 0\leq c\cdot g(n)\leq f(n)\forall n\geq n_0\}$ \newline \newline
		\textbf{Theorem.} Für zwei beliebige Funktionen $f(n)$ und $g(n)$ gilt: $f(n) = \Theta(g(n))$ genau dann wenn $f(n)=\BigO(g(n))$ und $f(n)=\Omega(g(n))$
		\subsection{Funktionsklassen}
		\begin{center}
			
		
		\begin{tabular} {c c}
			\hline
			g(n) & \textbf{Wachstum} \\
			\hline
			$1$ & konstant \\
			$\log n$ & logarithmisch \\
			$n$ & linear \\
			$n\log n$ & leicht überlinear \\
			$n^2$ & quadratisch \\
			$n^3$ & kubisch \\
			$n^k$ & polynomiell ($k$=Konstante) \\
			$2^n$ & exponentionell
		\end{tabular}
		\end{center} 
		\subsection{Rechenregeln}
		\subsubsection{Produktregel}
		$$f_1=\BigO(g_1),f_2=\BigO(g_2)\Rarr f_1\cdot f_2 =\BigO(g_1\cdot g_2)$$
		\subsubsection{Summenregel}
		$$f_1=\BigO(g_1),f_2=\BigO(g_2)\Rarr f_1+f_2=\BigO(g_1+g_2)$$
		\subsubsection{Multiplikation mit einer Konstante}		
		Die Konstante fällt weg
		\subsection{Funktionen Vergleichen}
		\subsubsection{Transitivität}
		\begin{align*}
			f(n)=\Theta(g(n)) \text{ und } g(n)=\Theta(h(n)) &\Rarr f(n)=\Theta(h(n)) \\
			f(n)=\BigO(g(n)) \text{ und } g(n)=\BigO(h(n)) &\Rarr f(n)=\BigO(h(n)) \\
			f(n)=\Omega(g(n)) \text{ und } g(n)=\Omega(h(n)) &\Rarr f(n)=\Omega(h(n))
		\end{align*}
		\subsubsection{Reflexivität}
		\begin{align*}
			f(n)&=\Theta(f(n))\\
			f(n)&=\BigO(f(n))\\
			f(n)&=\Omega(f(n)
		\end{align*}
		\subsubsection{Symmetrie}

		$$f(n)=\Theta(g(n))\Leftrightarrow g(n)=\Theta(f(n))$$
		\subsubsection{Transponierende Symmetrie}
		
		$$f(n)=\BigO(g(n))\Leftrightarrow g(n)=\Omega(f(n))$$
		\subsection{Rekursion, Rekurrenz und das Mastertheorem}
		\subsubsection{Rekursion}
		Zusätzliche Kosten durch Divide and Conquer: \newline
		\begin{enumerate}
			\item $T(n): $ Kosten für das Lösen der Instanz der Größe $n$
			\item $f(n): $ Kosten für das Divide einer Instanz der Größe $n$
			\item $g(n): $ Kosten für das Mergen von $k$ Teillösungen
		\end{enumerate}
		Daraus ergibt sich: 
		$$
			T(n) = k \cdot T(m) + f(n) + g(n)
		$$
		Für $T(m)$ ergibt sich:
		$$
			T(m) = k \cdot T(l) + f(m) + g(m)
		$$
		Einsetzen von $T(m)$ in $T(n)$:
		$$
			T(n) = k \cdot (k \cdot T(l) + f(m) + g(m)) + f(n) + g(n) = k^2 \cdot T(l) + k \cdot f(m)+ k \cdot g(m) + f(n) + g(n)
		$$
		\subsubsection{Mastertheorem}
			Gegebne sei eine Rekurrenz der Form $T(n) = aT(\frac{n}{b}+f(n)$ mit Konstanten $a\geq 1$ und $b\geq 1$ sowie einer beliebigen Funktion $f(n)$. $T(n)$ kann asymptotisch wie folgt beschränkt werden:
			\begin{enumerate}
				\item Falls $f(n)=\BigO(n^{\log_b(a)-\epsilon})$, für eine Konstante $\epsilon$, dann gilt $T(n)=\Theta(n^{log_ba})$
				\item Falls $f(n)=\Theta(n^{\log_b(a)})$, dann gilt $T(n)=\Theta (n^{\log_ba}\log n)$
				\item Falls $f(n)=\Omega(n^{\log_b(a)+\epsilon})$, für eine Konstante $\epsilon$ und falls $af(\frac{n}{b})\leq c\cdot f(n)$ für eine Konstante $c<1$ und ab einem $n_0>0$ für alle $n>n_0$, dann gilt $T(n)=\Theta(f(n)$
			\end{enumerate}
	\section{Sortieren}
	\subsection{Definitionen}
	Damit die Elemente sortiert werden können muss eine Ordungsrelation vorhanden sein: \newline \newline
	Es sei $R\subseteq AxA$ eine binäre Relation auf der Menge A.
	\begin{itemize}
		\item $R$ heißt Quasiordnung auf $A$ genau dann wenn $R$ reflexiv und transitiv ist
		\item $R$ heißt partielle Ordnung auf $A$ genau dann, wenn $R$ transitiv, reflexiv und antisymmetrisch ist
		\item $R$ auf $A$ heißt lineare Ordnung oder Totalordnung genau dann, wenn zusätzlich gilt: $\forall a, b\in A:R(a, b)\lor R(b, a)$
	\end{itemize}
	Ist $R$ eine Relation des Typs 1-3, schreibt man meist $a\leq_Rb$ oder $a\leq b$ \newline
	Wir betrachten ebenfalls sogenannte strikte Ordnungen bei denen ein Element niemals mit sich selbst in Relation  stehen darf. \newline \newline
	Es sei $R\subseteq AxA$ eine binäre Relation auf der Menge $A$.
	\begin{itemize}
	  \item $R$ heißt partielle Ordnung auf $A$ wenn $R$ irreflexiv und transitiv ist
	  \item $R$ auf $A$ heißt strikte Ordnung genau dann, wenn zusätzlich gilt: \newline
	  		$\forall a\not= b\in A:R(a,b)\lor R(b,a)$
	\end{itemize} 
	Für Sortierverfahren braucht es eine strenge schwache Ordnung: \newline \newline
	Es sei $R\subseteq AxA$ eine binäre Relation auf der Menge $A$. $R$ ist eine strenge schwache Ordnung wenn $R$ eine strikte Ordnung und diese zusätzlich negativ transitiv ist.
	\newpage
	\subsection{Vergleichsbasierte Algorithmen}
	Brauchen im worst-case immer mindestens $\log(n!)=\Theta(n\log(n))$ Vergleiche
	\subsubsection{Insertionsort}
	Idee: Füge das aktuelle Element an die richtige Position im sortierten Teil ein \newline \newline
	Best case: $\BigO(n)$ \newline
	Worst case: $\BigO(n^2)$
	\subsubsection{Bogosort}
	Idee: wähle eine zufällige Permutation und überprüfe, ob diese sortiert ist \newline \newline
	Best case: $\BigO(n)$ \newline
	Average case: $\BigO(e-1)n!$ \newline
	Worst case: $\BigO(n\cdot n!)$
	\subsubsection{Slowsort}
	Idee: Verzögere die Sortierung so lange es geht \newline \newline
	Best case: $\BigO(n\frac{\log(n)}{2+e})$ \newline
	Average case: $\BigO(n\frac{\log(n)}{2+e})$ \newline
	Worst case:$\BigO(n\frac{\log(n)}{2+e})$
	\subsubsection{Bubblesort}
	Idee: Das kleinste Element steigt wie eine Blase auf, dabei werden die Elemente paarweise verglichen \newline \newline
	Best case: $\BigO(n)$ \newline
	Worst case: $\BigO(n^2)$
	\subsubsection{Mergesort}
	Idee: Slowsort ist so langsam, weil es brauchbare Zwischenergebnisse ignoriert. Anstatt nur die beiden  letzten Elemente der über Rekursion erhaltenen sortierten Teile zu vergleichen, fügt man die Teile zusammen, wobei man die Sortierung erhält \newline \newline
	Rekurrenzgleichung: $T(n)=2\cdot T(\frac{n}{2}+\Theta(n))$ \newline
	Daraus folgt mit dem Mastertheorem (2. Fall): $T(n)=\Theta(n\log n)$
	\subsubsection{Selectionsort}
	Idee: Selectionsort sucht für die aktuelle Position das geeignete Element, nämlich  das kleinste (bzw. größte) im unsortierten Teil \newline \newline
	Laufzeit: $\Theta(n^2)$
	\subsubsection{Heapsort}
	Idee: Nutzte max-heap.Eigenschaft eines Heaps, da Maximum an erster Stelle \newline \newline
	Laufzeit: $\BigO(n\log n$ 
	\subsubsection{Quicksort}
	Idee: 
	\begin{itemize}
	  \item Divide: teile an Pivotelement in linke Hälfte, rechte Hälfte und Pivotelement, nach Partitionsschritt
	  \item Conquer: rekursiv auf Hälften anwenden
	  \item Combine: Zusammenfügen der einzelnen Elemente
	\end{itemize}
	Worst case: $\Theta(n^2)$ \newline
	Best case: $\Theta(n\log n)$
	\subsection{Nicht vergleichsbasierte Algorithmen}
	Durch lösen von der Beschränkung auf Vergleiche schneller werden
	\subsubsection{Bucketsort}
	Idee: 
	\begin{enumerate}
	  \item Bucketsort nimmt an, dass die Elemente des zu sortierenden Arrays A im Intervall $[0,1)$ gleich verteilt sind
	  \item Bucketsort teilt das Intervall $[0,1)$ in $n$ (Länge von A) gleich große Teilintervalle ("Buckets")
	  \item Anschließend werden die $n$ Elemente von $A$ in die Buckets verteilt
	  \item Nichtleere Buckets werden sortiert
	  \item Anschließend wird das Ergebnis aus den Buckets zusammengestzt
	\end{enumerate}
	Best case: $\Theta(n)$ \newline
	Average case: $\Theta(n)$ \newline
	Worst case: $\Theta(n^2)$
	\subsubsection{Countingsort}
	Idee: 
	\begin{itemize}
	  \item Nimmt an, dass jedes der $n$ Elemente des zu sortierenden Arrays $A$ eine Ganzzahl zwischen $0$ und $k$ ist
	  \item Für jedes Element $x$ aus dem Array $A$ wird zunächst die Anzahl an Elementen bestimmt, die $\leq x$ sind
	  \item Diese Information wird dann genutzt um das Element $x$ direkt an seine richtige Position im sortierten Array zu plazieren
	\end{itemize}
	Laufzeit: $\Theta(n+k)$
	\subsubsection{Radixsort}
	Idee: Sortiere die Elemente im sortierenden Array $A$ Ziffer für Ziffer, beginne bei Least Significant Digit \newline \newline
	Laufzeit: $\Theta(d\cdot(n+k))$ für $n$ $d$-stellige Zahlen
	\section{Graphen}
	\subsection{Definitionen}
	\subsubsection{Graph}
		Sei $V=\{v_1,...,v_n\}$ eine endliche Menge und $E\subseteq P_2(V)=\{\{u,v\}|u,v\in V,u\not = v\}$. Dann heißt das geordnete Paar $G=(V,E)$ (endlicher, schlichter, ungerichteter) Graph, wobei $V$ die Knotenmenge und $E$ die Kantenmenge von $G$ ist. Ist $e=\{u,v\}\in E$ so sind $u$ und $v$ benachbart (adjazent). $e=\{u,v\}$ oder auch $u-v$ ist dabei eine Kante 
	\subsubsection{Vollständiger Graph}
		Ein Graph heißt vollständig, wenn jede Ecke mit jeder anderen Ecke durch genau eine Kante verbunden ist. $K_n$ bezeichnet den vollständigen Graphen mit $n$ Ecken
	\subsubsection{Grad}
		Der Grad eines Knoten in $G=(V,E)$ ist die Anzahl an benachbarten Knoten. Der Grad eines Graphen entspricht dem maximalen Grad eines enthaltenen Knotens
	\subsubsection{Regulärer Grad}
		Ein Graph heißt regulär, wenn alle Knoten des Graphen den gleichen Grund haben
	\subsubsection{Teilgraph}
		Seien $G$ und $H$ Graphen. $H$ heißt Teilgraph $A$ von $G$, falls $V(H)\subseteq V(G)$ und $E(H)\subseteq E(G)$ gilt
	\subsubsection{Wege und Kreise}
		Ein Weg (auch Pfad genannt) ist eine Folge von Kanten. Ein einfacher Weg besucht keine Knoten doppelt. Ein Kreis ist ein einfacher Weg, wobei Anfangs- und Endknoten äquivalent sind. Ein einfacher Kreis besucht keine Knoten mehrfach
		\newpage
	\subsubsection{Kürzeste Distanz}
		Sei $G=(V,E)$ ein Graph und seien $u,v\in V$ Knoten. Die kürzeste Distanz $\delta(u,v)$ von Knoten $u$ nach Knoten $v$ ist die minimale Anzahl an Knoten in einem Pfad von $u$ nach $v$. Wenn kein Pfad von $u$ nach $v$ existiert, so ist $\delta(u,v)=\infty$. Ein Pfad $\delta(u,v)$ vom $u$ nach $v$ wird als kürzester Pfad bezeichnet
	\subsubsection{Zusammenhängend}
		Graph $G=(V,E)$ heißt zusammenhängend, falls zwischen je zwei Knoten $u,v\in V$ ein Weg existiert
	\subsubsection{Wald}
		Graph $G=(V,E)$ ist ein Wald, falls $G$ keine einfachen Kreise enthält
	\subsubsection{Baum}
		Graph $G=(V,E)$ ist ein Wald, falls G ein zusammenhängender Wald ist
	\subsubsection{Disjunktionen}
		Zwei Graphen sind disjunkt, wenn es keine Weg von dem einen in den anderen gibt. Zwei Wege sind disjunkt, wenn es keinen Knoten gibt, der in beiden enthalten ist
	\subsubsection{Bipartierter Graph}
		Ein einfacher Graph $G=(V,E)$ heißt bipartit, falls sich seine Knoten in zwei disjunkten Teilmengen $A$ und $B$ aufteilen lassen, sodass zwischen den Knoten innerhalb beider Teilmengen keine Kanten verlaufen
	\subsubsection{Zusammenhängende Digraphen}
		Digraph $G=(V,E)$ heißt stark zusammenhängend, falls für je zwei Knoten $u,v\in V$ gilt: Es gibt einen einfachen Weg von $u$ nach $v$ und von $v$ nach $u$. Falls $G$ ungerichtet zusammenhängend ist, ist G schwach zusammenhängend
	\subsection{Algorithmen}
	\subsubsection{Breitensuche}
		Idee: 
		\begin{itemize}
		  \item Breitensuche arbeitet iterativ und läuft Level für Level durch den Graphen
		  \item Level für Level bedeutet intuitiv, dass, gegeben ein Startknoten, sich der Algorithmus erst alle Nachbarn des Startknoten anschaut, bevor er die Nachbarn der Nachbarn anschaut, usw.
		\end{itemize}
		Laufzeit: $\BigO(|V|+|E|)$
	\subsubsection{Tiefensuche}
		Idee: Tiefensuche arbeitet im Gegensatz zur Breitensuche nicht Level für Level, sondern steigt immer so weit es geht in die Tiefe
	\subsubsection{Dijkstra}
		Idee:
		\begin{itemize}
		  \item Generalisiert BFS für gewichtete Graphen
		  \item Relaxierugn: Die Pfadlänge zu einem Knoten wird geschätzt und während des Durchlaufs aktualisiert 
		  \item Initial werden die Kosten der nicht zu erreichenden Knoten auf sehr hohe Werte gesetzt (für den Startknoten auf 0) 
		  \item Die Relaxoerung wird so lange. durchgeführt, bis keine Aktualisierung mehr möglich ist
		\end{itemize}
		Laufzeit: $\BigO(|V|^2+|E|)$
	\subsubsection{Floyd-Warshall}
		Idee:
		\begin{itemize}
		  \item Dynamische Programierung
		  \item Wähle einen Knoten $v$ aus und betrachte ob es über $v$ einen kürzeren Weg zwischen allen andern Knotenpaaren gibt: Aktualisiere gegebenenfalls 
		  \item Wiederhole dies für alle Knoten
		\end{itemize}
		Laufzeit: $\Theta(|V|^3)$
	\subsubsection{Page Rank}
	
\end{document}


















